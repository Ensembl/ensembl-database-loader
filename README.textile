h1. Ensembl Mirrors Pipeline

This is a re-implementation of the mirroring pipeline using the Hive infrastructure. This allows for a queue based scheduler to run the code as well as a local machine to load a database with automatic retry. For more information about Hive please consult http://www.ensembl.org/info/docs/eHive/index.html.

h1. Installing

This pipeline requires:

* A checkout of Ensembl core and hive (v55 minimum)
** Please see http://www.ensembl.org/info/docs/eHive/installation.html
** The ENSEMBL_CVS_ROOT_DIR environment variable set to the checkout directory
* BioPerl 1.2.3
* A database for hive
** SQLite is preferred but MySQL can be configured
* A MySQL database server (5.1 minimum) for Ensembl
* Perl dependencies
** Perl 5.8.9 (minimum)
** Net::FTP
** File::Spec
** IO::File
** IO::Uncompress::Gzip
* Binary dependencies
** mysql binary

h1. Running the Pipeline

h2. Selecting your databases & versions

By default the pipeline will load the API version of the core API you are using. If you want a full mirror than you can specify a different version using the <code>-release</code> command line argument.

If a subset of databases is required then create a file with one database per line and give this to the program using the <code>-required_dbs_file</code> parameter. For example to load a portion of human databases

<pre>
homo_sapiens_core_64_37
homo_sapiens_rnaseq_64_37
homo_sapiens_vega_64_37
</pre>

h2. Working with Ensembl Genomes

You can download and host Ensembl Genomes databases by specifying the <code>-division</code> tag to flag which division we should look for the server in. The meaning of <code>-release</code> also changes to flag the *Ensembl Genomes release* i.e. release *10* not release 63. The functionality is untested but should work.

h2. Running the pipeline

h3. Environment Variables

h4. ENSEMBL_CVS_ROOT_DIR

This should be set to the root of the directory for your ensembl checkouts. The directories must be named as they come out of CVS checkouts.

h4. PERL5LIB

This should be set to the following:

<pre>
PERL5LIB=$ENSEMBL_CVS_ROOT_DIR/ensembl-database-loader/modules:$PERL5LIB
PERL5LIB=$ENSEMBL_CVS_ROOT_DIR/ensembl/modules:$PERL5LIB
PERL5LIB=$ENSEMBL_CVS_ROOT_DIR/ensembl-hive/modules:$PERL5LIB
PERL5LIB=$ENSEMBL_CVS_ROOT_DIR/bioperl-live:$PERL5LIB
export PERL5LIB
</pre>

h4. PATH

Set to the following

<pre>export PATH=$ENSEMBL_CVS_ROOT_DIR/ensembl-hive/scripts:$PATH</pre>

h4. Database and FTP Params

To make the guide easier we will export the settings for the target database. This is not a requirement but means you should be able to copy paste examples from this document out.

<pre>
FTP_PASS='my@email.com'
DB_HOST='host.mysql'
DB_PORT=3306
DB_USER='user'
DB_PASS='password'
</pre>

h2. Example Setups 

h3. Current Checked-out API Release

<pre>
usr@srv $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl-database-loader
usr@srv $ init_pipeline.pl Bio::EnsEMBL::DBLoader::PipeConfig::LoadDBs_conf \
  -target_db_host $DB_HOST -target_db_port $DB_PORT -target_db_user $DB_USER -target_db_pass $DB_PASS 
</pre>

h3. Version Specific Databases

<pre>
usr@srv $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl-database-loader
usr@srv $ init_pipeline.pl Bio::EnsEMBL::DBLoader::PipeConfig::LoadDBs_conf \
  -target_db_host $DB_HOST -target_db_port $DB_PORT -target_db_user $DB_USER -target_db_pass $DB_PASS \
  -release 64
</pre>

h3. Ensembl Genomes Databases

<pre>
usr@srv $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl-database-loader
usr@srv $ init_pipeline.pl Bio::EnsEMBL::DBLoader::PipeConfig::LoadDBs_conf \
  -target_db_host $DB_HOST -target_db_port $DB_PORT -target_db_user $DB_USER -target_db_pass $DB_PASS \
  -division metazoa \
  -release 10
</pre>

h3. List of Databases

<pre>
usr@srv $ cd $ENSEMBL_CVS_ROOT_DIR/ensembl-database-loader
usr@srv $ init_pipeline.pl Bio::EnsEMBL::DBLoader::PipeConfig::LoadDBs_conf \
  -target_db_host $DB_HOST -target_db_port $DB_PORT -target_db_user $DB_USER -target_db_pass $DB_PASS \
  -required_dbs_file my.dbs
</pre>

h2. Running the Pipeline

Hive will tell you to run a number of commands. The sync process is very important to run and means the setup will work. If you are running the code on a non-LSF system then specify local to run all commands locally. The code is quite memory light but processor intensive so please be aware of the ramifications of running a local pipeline.

